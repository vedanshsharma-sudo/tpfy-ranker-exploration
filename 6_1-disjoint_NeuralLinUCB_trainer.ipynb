{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1f223b-b206-4e47-a8a2-3525aaba18fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 13:35:44.767551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2026-02-13 13:35:44.767579: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:dynamic_embedding.GraphKeys has already been deprecated. The Variable will not be added to collections because it does not actully own any value, but only a holder of tables, which may lead to import_meta_graph failed since non-valued object has been added to collection. If you need to use `tf.compat.v1.train.Saver` and access all Variables from collection, you could manually add it to the collection by tf.compat.v1.add_to_collections(names, var) instead.\n",
      "WARNING: env TENANT is not set, use the default value apse1-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 13:35:45.748181: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2026-02-13 13:35:45.748205: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2026-02-13 13:35:45.748220: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-11-72-173): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['ENV'] = 'prod'\n",
    "os.environ['REGION'] = 'apse1'\n",
    "os.environ['TENANT'] =\"in\"\n",
    "os.environ['RECO_S3_BUCKET'] = \"p13n-reco-offline-prod\"\n",
    "os.environ['COUNTRY_KEY']= \"in\"\n",
    "os.environ['AWS_REGION']= \"ap-southeast-1\"\n",
    "os.environ['USE_REAL_CMS3']= \"True\"\n",
    "os.environ['RECO_CREDENTIAL']= \"-----BEGINRSAPRIVATEKEY-----\\nMGICAQACEQCdHOlGnxIMWCMzjK2JAg37AgMBAAECEGOIwGTEO9vd3X9+jyiF4NECCQnoqDakDgSm2QIID9sadWN0XvMCCQLiqPkgVKSuIQIIDCAsWM+pJB8CCQG0jbIGCNX9MA==\\n-----ENDRSAPRIVATEKEY-----\"\n",
    "\n",
    "import argparse, gc\n",
    "import json\n",
    "import os, time\n",
    "import numpy as np\n",
    "import s3fs, pickle\n",
    "import pyarrow\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "tfv1 = tf.compat.v1\n",
    "tfv1.disable_v2_behavior()\n",
    "\n",
    "# Enable memory growth for GPUs to avoid memory fragmentation\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_recommenders_addons as tfra\n",
    "from model.losses import masked_binary_entropy_loss\n",
    "from model.metrics import MaskedAUC\n",
    "\n",
    "from common.config.utils import data_path, model_path\n",
    "from common.config import TENANT\n",
    "from tpfy.tf_model.tpfy_model_v3_mtl import TpfyModelV3, TpfyMtlModelConfig\n",
    "from tpfy.etl.schema import TpfyMtlDatasetSchema\n",
    "from model.parquet_dataset import TFParquetDataset\n",
    "from tpfy.common import TpfyDataPath\n",
    "from omegaconf import OmegaConf\n",
    "from dataclasses import dataclass\n",
    "from tpfy.train_v3_mtl import make_example_mtl, TpfyTrainConfig, TpfyConfig\n",
    "from tpfy.helper import load_model_weights_from_s3\n",
    "\n",
    "def create_dataset(date, path = None):\n",
    "    if path:\n",
    "        data_path_str = path\n",
    "    else:\n",
    "        data_path_str = data_path(\n",
    "            TpfyDataPath.S3_TPFY_IMPR_V3_DAILY_MTL_EXTRACTED_EXAMPLES, TENANT\n",
    "        ) % (variant, date)\n",
    "\n",
    "    dataset = TFParquetDataset([data_path_str], TpfyMtlDatasetSchema, shuffle_files=True)\n",
    "    tf_dataset = dataset.create_tf_dataset(batch_size).map(make_example_mtl)\n",
    "    return tf_dataset\n",
    "\n",
    "def load_and_compile_model():\n",
    "    # Build model\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BUILDING MODEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    model = TpfyModelV3(\n",
    "        hparams.model,\n",
    "        click_ns=args.click_ns,\n",
    "        enable_random_watch=hparams.train.enable_random_watch,\n",
    "    )\n",
    "\n",
    "    # Create optimizer (needed for compilation, even though we won't train)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        weight_decay=0.0,  # Not needed for inference\n",
    "        learning_rate=0.001,  # Not needed for inference\n",
    "        epsilon=1e-4,\n",
    "    )\n",
    "\n",
    "    loss_dict = {\n",
    "            \"click\": masked_binary_entropy_loss(from_logits=True),\n",
    "            \"watch\": masked_binary_entropy_loss(from_logits=True),\n",
    "            \"random_watch\": masked_binary_entropy_loss(from_logits=False),\n",
    "            \"paywall_view\": masked_binary_entropy_loss(from_logits=True),\n",
    "            \"add_watchlist\": masked_binary_entropy_loss(from_logits=True),\n",
    "    }\n",
    "    metric_dict = {\n",
    "            \"click\": MaskedAUC(from_logits=True),\n",
    "            \"watch\": MaskedAUC(from_logits=True),\n",
    "            \"random_watch\": MaskedAUC(from_logits=False),\n",
    "            \"paywall_view\": MaskedAUC(from_logits=True),\n",
    "            \"add_watchlist\": MaskedAUC(from_logits=True),\n",
    "    }\n",
    "\n",
    "    optimizer = tfra.dynamic_embedding.DynamicEmbeddingOptimizer(optimizer)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_dict, metrics=metric_dict)\n",
    "    print(\"Model compiled\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_activations_and_labels(next_batch, last_layer_tensor):\n",
    "    features, labels, metadata = session.run(next_batch)\n",
    "    \n",
    "    activation_values = session.run(\n",
    "        last_layer_tensor,\n",
    "        feed_dict={} if not features else None  \n",
    "    )\n",
    "    \n",
    "    return activation_values, labels, metadata\n",
    "\n",
    "def compute_A_b(A, b, arm_index, next_batch, last_layer_tensor):\n",
    "    def process_batch_fast(content_ids, embeddings, labels, arm_index, A, b):\n",
    "        unique_ids, inverse = np.unique(content_ids, return_inverse=True)\n",
    "\n",
    "        # ensure all arms exist\n",
    "        for cid in unique_ids:\n",
    "            if cid not in arm_index:\n",
    "                arm_index[cid] = len(A)\n",
    "                A.append(lambda_reg * np.eye(d))\n",
    "                b.append(np.zeros(d))\n",
    "\n",
    "        # group outer products\n",
    "        outer = embeddings[:, :, None] * embeddings[:, None, :]  # (B, d, d)\n",
    "        xb = embeddings * labels[:, None]\n",
    "        \n",
    "        for u_idx, cid in enumerate(unique_ids):\n",
    "            global_idx = arm_index[cid]\n",
    "            mask = (inverse == u_idx)\n",
    "\n",
    "            A[global_idx] += outer[mask].sum(axis=0)\n",
    "            b[global_idx] += xb[mask].sum(axis=0)\n",
    "        \n",
    "    activation_values, labels, metadata = get_activations_and_labels(next_batch, last_layer_tensor)\n",
    "    y_batch = labels['click']\n",
    "    mask = (y_batch != -1)\n",
    "\n",
    "    if not np.any(mask):\n",
    "        del activation_values, labels, metadata, mask\n",
    "        return A\n",
    "\n",
    "    H = activation_values[mask.squeeze()].copy() \n",
    "    y_batch = y_batch[mask].reshape(sum(mask)[0], )\n",
    "    content_id = [content_id for content_id, mask_bool in zip(metadata['content_id'], mask) if mask_bool]    \n",
    "    \n",
    "    del activation_values, mask  # Delete immediately\n",
    "    \n",
    "    H = H / (np.linalg.norm(H, axis=1, keepdims=True) + 1e-8)\n",
    "    # A += H.T @ H\n",
    "    process_batch_fast(content_id, H, y_batch, arm_index, A, b)\n",
    "\n",
    "    del H, labels, metadata, content_id, y_batch\n",
    "    return A, b, arm_index\n",
    "\n",
    "def run(args):\n",
    "    tf_dataset = create_dataset(args.date)\n",
    "    iterator = tf_dataset.make_one_shot_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "    sample_features, sample_labels, sample_metadata = session.run(next_batch)\n",
    "    tpfy_model = load_and_compile_model()\n",
    "\n",
    "    prediction = tpfy_model(sample_features, training=False)\n",
    "    session.run([\n",
    "        tfv1.global_variables_initializer(),\n",
    "        tfv1.local_variables_initializer(),\n",
    "        tfv1.tables_initializer()\n",
    "    ])\n",
    "\n",
    "    plain_weights = load_model_weights_from_s3(\n",
    "        args.model_name,\n",
    "        use_s3=True,\n",
    "        checkpoint_name=args.checkpoint\n",
    "    )\n",
    "    plain_weights_modified = {k.replace('train/', ''): v for k, v in plain_weights.items()}\n",
    "    restore_ops = tpfy_model.restore_plain_weights_ops(\n",
    "        plain_weights_modified,\n",
    "        clear_nn=args.clear_nn\n",
    "    )\n",
    "    session.run(restore_ops)\n",
    "\n",
    "    # Create NEW iterator (reset to start of dataset)\n",
    "    iterator = tf_dataset.make_one_shot_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "    # Get compress_output tensor (linear_input)\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    compress_output_tensor = graph.get_tensor_by_name(f'tpfy_model_v3/deepfm/{args.layer_name}:0')\n",
    "    \n",
    "    d = 128\n",
    "    lambda_ = 1\n",
    "    arm_index = {}          # content_id -> row index in A\n",
    "    A = []\n",
    "    b = []\n",
    "\n",
    "    start = time.time()\n",
    "    run_ = 0\n",
    "    os.makedirs(f'disjoint_neural_linucb/disjoined_neural_linUCB_offline_matrices_{args.date}_{args.checkpoint}', exist_ok=True)\n",
    "    while True:\n",
    "        if (run_ % 100 == 0) and (run_):\n",
    "            gc.collect()\n",
    "            print(f'Length of arm index : {len(arm_index)}')\n",
    "            print(f'Run {run_} completed in {time.time() - start} s!')\n",
    "            start = time.time()\n",
    "        if (run_ % 1000 == 0) and (run_):\n",
    "            np.save(f'disjoint_neural_linucb/disjoined_neural_linUCB_offline_matrices_{args.date}_{args.checkpoint}/A_{run_}.npy', A)\n",
    "            np.save(f'disjoint_neural_linucb/disjoined_neural_linUCB_offline_matrices_{args.date}_{args.checkpoint}/b_{run_}.npy', b)\n",
    "            with open(f'disjoint_neural_linucb/disjoined_neural_linUCB_offline_matrices_{args.date}_{args.checkpoint}/arm_index_{run_}.pkl', 'wb') as handle:\n",
    "                pickle.dump(arm_index, handle)\n",
    "            gc.collect()\n",
    "            print(f'Run {run_} completed in {time.time() - start} s!')\n",
    "            start = time.time()\n",
    "        try:\n",
    "            A, b, arm_index = compute_A(A, b, arm_index, next_batch, compress_output_tensor)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset reached\")\n",
    "            break\n",
    "        run_ += 1\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"TPFY Exploration offline Training.\")\n",
    "    parser.add_argument(\"model_name\", type=str)\n",
    "    parser.add_argument(\"date\", type=str)\n",
    "    parser.add_argument(\"--click_ns\", type=float, default=0.08)\n",
    "    parser.add_argument(\"--variant\", type=str, default=\"cms3\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "    parser.add_argument(\"--clear_nn\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--checkpoint\", default=None, type=str)\n",
    "    parser.add_argument(\"--layer_name\", default='Relu', type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load configuration\n",
    "    config_name = f\"tpfy/tpfy_config/mtl-{TENANT}.yaml\"\n",
    "    if not os.path.exists(config_name):\n",
    "        raise FileNotFoundError(f\"Config file {config_name} not found\")\n",
    "\n",
    "    hparams: TpfyConfig = OmegaConf.merge(\n",
    "        OmegaConf.structured(TpfyConfig),\n",
    "        OmegaConf.load(config_name),\n",
    "    )\n",
    "    print(f\"\\nLoaded config: {config_name}\")\n",
    "\n",
    "    # Override batch size if specified\n",
    "    if args.batch_size:\n",
    "        hparams.train.batch_size = args.batch_size\n",
    "\n",
    "    batch_size = hparams.train.batch_size\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "    # Load dataset\n",
    "    variant = args.variant\n",
    "    if variant and not variant.startswith(\"-\"):\n",
    "        variant = \"-\" + variant\n",
    "\n",
    "    session = tfv1.keras.backend.get_session()\n",
    "    print(\"Start training\")\n",
    "    run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e1a95ea5-24e9-4845-b62e-e619d65f9fcb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.load('disjoint_neural_linucb/disjoined_neural_linUCB_offline_matrices_2026-02-09_1770723470/A_2000.npy')\n",
    "b = np.load('disjoint_neural_linucb/disjoined_neural_linUCB_offline_matrices_2026-02-09_1770723470/b_2000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "314f1c98-f5d8-4533-af64-9bdf3c7d46b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6884, 128, 128), (6884, 128))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8ee49d25-e2c3-4c0b-9591-ea606ab571aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tpfy-v3-mtl-r2'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cd1b330b-3bfd-490d-b8f6-daa577ab5143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will upload to S3: s3://p13n-reco-offline-models-prod/models/tpfy/tpfy-v3-neural-linucb/2026-02-09\n"
     ]
    }
   ],
   "source": [
    "s3_base_path = model_path(\n",
    "    f\"tpfy/tpfy-v3-neural-linucb/{args.date}\",\n",
    "    TENANT\n",
    ")\n",
    "print(f\"Will upload to S3: {s3_base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0192514a-7faf-460c-89f1-f62e191a7017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00008-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20709-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00016-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20696-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00017-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20681-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00020-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20691-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00019-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20693-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00004-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20698-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00031-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20702-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00009-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20678-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00022-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20694-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00002-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20686-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00021-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20682-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00018-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20697-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00030-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20703-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00027-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20707-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00025-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20688-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00001-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20692-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00010-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20700-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00014-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20701-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00005-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20699-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00012-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20689-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00000-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20679-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00026-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20683-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00006-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20705-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00015-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20706-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00029-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20685-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00003-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20680-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00023-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20687-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00007-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20704-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00028-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20684-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00013-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20690-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00024-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20695-1-c000.snappy.parquet,s3://p13n-reco-offline-prod/dataset_v5/tpfy-impr-v3/daily-mtl-extracted-cms3/cd=2026-02-09/part-00011-tid-501209436517690488-f33627ca-41ea-4b1f-ad53-c02fcbc181ee-20708-1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "data_path_str = data_path(\n",
    "    TpfyDataPath.S3_TPFY_IMPR_V3_DAILY_MTL_EXTRACTED_EXAMPLES, TENANT\n",
    ") % (f'-{args.variant}', args.date)\n",
    "\n",
    "dataset = TFParquetDataset([data_path_str], TpfyMtlDatasetSchema, shuffle_files=True)\n",
    "tf_dataset = dataset.create_tf_dataset(batch_size).map(make_example_mtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "01a51780-e2f2-4528-947c-3672984c723d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30685/3633043408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2911\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    489\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "for batch in tf_dataset:\n",
    "    print(batch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34b229-2b3f-42eb-8d0a-100cba234f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
